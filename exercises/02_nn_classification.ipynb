{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a binary classification dataset with Scikit-Learn's ```make_moons()``` function.\n",
    "- For consistency, the dataset should have 1000 sampes and 42 as random state.\n",
    "- Turn the data into PyTorch tensors. Split the data into training and test sets using ```train_test_split``` with 80% training and 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "x_samples, y_samples = make_moons(n_samples=1000, random_state=42)\n",
    "\n",
    "x_samples, y_samples = torch.tensor(x_samples, dtype=torch.float), torch.tensor(y_samples, dtype=torch.float)\n",
    "x_train, x_test, y_train,  y_test = train_test_split(x_samples, y_samples, train_size=0.8, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build a model by subclassing ```nn.Module``` that incorporates non-linear activation functions and is capable of fitting the data you created in 1.\n",
    "- Feel free to use any combination of PyTorch layers (linear and non-linear) you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ccNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(2, 10)\n",
    "        self.linear_2 = nn.Linear(10, 10)\n",
    "        self.linear_3 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = ccNet().to(device)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Setup a binary classification compatible loss function and optimizer to use when training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a training and testing loop to fit the model you created in 2 to the data you created in 1.\n",
    "- To measure model accuracy, you can create your own function or use the accuracy function in TorchMetrics.\n",
    "- Train the model for long enough for it to reach over 96% accuracy.\n",
    "- The training loop should output progress every 10 epochs of the model's training and test set loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "torch.manual_seed(42)\n",
    "epochs = 1000\n",
    "acc_fn = torchmetrics.Accuracy().to(device)\n",
    "\n",
    "x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ## Training\n",
    "    model.train()\n",
    "\n",
    "    train_logits = model(x_train).squeeze()\n",
    "    train_preds = torch.round(torch.sigmoid(train_logits))\n",
    "\n",
    "    train_loss = loss_fn(train_logits, y_train)\n",
    "    train_acc = acc_fn(y_train, train_preds.int())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        test_logits = model(x_test).squeeze()\n",
    "        test_preds = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = acc_fn(y_test, test_preds.int())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss:.5f}, Accuracy: {train_acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make predictions with your trained model and plot them using the ```plot_decision_boundary()``` fucntion created in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "\n",
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot decision boundaries for traininig and test sets\n",
    "plt.figure(figsize =(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Train')\n",
    "plot_decision_boundary(model, x_train, y_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Test')\n",
    "plot_decision_boundary(model, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Replicate the Tanh (hyperbolic tangent) activation frunction in pure PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create a multi-class dataset using the spirals data creation function from CS231n\n",
    "- Construct a model capable of fitting the data (you may need a combination of linear and non-linear layers).\n",
    "- Build a loss function and optimizer capable of handling multi-class data (optional: use Adam optimizer instead of SGD)\n",
    "- Make a training and testing loop for the multi-class data and train a model on it to reach over 95% testing accuracy .\n",
    "- Plot the decision boundaries on the spirals dataset from your model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for creating a spiral dataset from CS231n\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "X = np.zeros((N*K, D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "for j in range(K):\n",
    "    ix = range(N*j, N*(j+1))\n",
    "    r = np.linspace(0.0, 1, N) # radius\n",
    "    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N) * 0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j\n",
    "\n",
    "# lets visualize the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = torch.tensor(X, dtype=torch.float).to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ccNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(2, 10)\n",
    "        self.linear_2 = nn.Linear(10, 10)\n",
    "        self.linear_3 = nn.Linear(10, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = ccNet().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "acc_fn = torchmetrics.Accuracy().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_logits = model(X_train)\n",
    "    train_preds = torch.argmax(train_logits, dim=1)\n",
    "\n",
    "    train_loss = loss_fn(train_logits, y_train)\n",
    "    train_acc = acc_fn(y_train, train_preds)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(X_test).squeeze()\n",
    "        test_preds = torch.argmax(test_logits, dim=1)\n",
    "\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = acc_fn(y_test, test_preds)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch} | Train Loss: {train_loss} | Train Acc: {train_acc:.2f} | Tests Loss: {test_loss}, | Test Acc: {test_acc:.2f}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Train')\n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Test')\n",
    "plot_decision_boundary(model, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write down 3 problems where you think machine classification could be useful\n",
    "\n",
    "    1. Medical diagnosis: Machine classification could be used to accurately diagnose diseases or conditions based on a patient's symptoms, test results, and other relevant information. This could potentially improve the accuracy and speed of diagnoses, and allow doctors to focus on more complex and nuanced aspects of patient care.\n",
    "    2. Spam filtering: Machine classification could be used to automatically identify and filter out spam emails, based on patterns and characteristics that are typical of spam messages. This could help individuals and organizations reduce the amount of time and effort they spend sorting through and deleting unwanted emails.\n",
    "    3. Sentiment analysis: Machine classification could be used to analyze text data, such as social media posts or online reviews, in order to determine the sentiment expressed in the text. For example, a classifier could be trained to identify whether a tweet or review is positive, negative, or neutral, based on the words and phrases used. This could be useful for companies looking to gauge public opinion about their products or services, or for researchers studying trends in public sentiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Research the concept of \"momentum\" in gradient-based optimizers (like SGD or Adam), what does it mean?\n",
    "\n",
    "\n",
    "In gradient-based optimization algorithms, momentum is a term used to describe the accumulation of past gradients, which can help the optimizer make faster and more stable progress towards a minimum of a loss function.\n",
    "\n",
    "In the case of stochastic gradient descent (SGD), momentum can be incorporated by adding a fraction of the previous update to the current update. This can help the optimizer escape from local minima or saddle points, and can also help the optimizer converge faster by allowing it to take larger steps in the direction of the minimum.\n",
    "\n",
    "In the Adam optimization algorithm, momentum is implemented in a similar way, but it is called the \"exponential moving average of the gradient\" and is denoted by the symbol v. The moving average is calculated as a weighted average of the past gradients, with the weighting decaying exponentially over time. The Adam optimizer also uses another parameter, called the \"exponential moving average of the squared gradient,\" which is denoted by the symbol s. Together, v and s are used to adaptively adjust the learning rate for each parameter, based on the historical gradient and curvature information.\n",
    "\n",
    "Overall, the use of momentum in gradient-based optimization algorithms can help the optimizer make more stable and efficient progress towards a minimum of the loss function, especially in cases where the loss function has a complex or noisy landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiteacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "645d004dd1412f554783dad564b4ed89d297d59ce6765071e5ea4608253deff6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
