{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What are 3 areas on industry where computer vision is currently being used?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Streaming services\n",
    "2. Social network services\n",
    "3. Image generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Search \"what is overfitting in machine learning?\" and write down a sentence about what you find."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model is excessively complex and has too many parameters relative to the size of the training data. As a result, the model may fit the training data very well, but it may no generalize well to new, unseen data. This means that the model will perform poorly on tasks it has not seen before, even though it may have a high accuracy on the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Search \"ways to prevent overfitting in machine learning\" write down 3 of the things you find and a sentence about each."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use cross-validation: Instead of splitting your data into a fixed training and testing set, you can use cross-validation to evaluate your model. This involves dividing your data into \"folds\", training on some folds, and evaluating on the remaining folds. You can then average the performance across all folds to get a better estimate of your model's generalization performance.\n",
    "2. Ensemble methods: One way to reduce overfitting is to train multiple models and combine their predictions. This can be done by averaging the predictions of multiple models, or by training a higher-level model to make use of the predictions of multiple lower-level models.\n",
    "3. Reducing the complexity of the model: A model with too many parameters may be prone to overefitting. One way to prevent this is to use a simpler model with fewer parameters, or to use techniques like feature selection to remove unnecessary features from the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Spend 20-minutes reading and clicking through the CNN Eplainer website"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. load the ```torchvision.datasets.MNIST``` train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize at leat 5 different samples of the MNIST training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "row=1\n",
    "column=5\n",
    "for i in range(5):\n",
    "    image, label = train_data[i]\n",
    "    plt.subplot(row,column,i+1)\n",
    "    plt.title(label)\n",
    "    plt.axis(False)\n",
    "    plt.imshow(image.squeeze(), cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Turn the MNIST train and test datasets into dataloaders using ```torch.utils.data.DataLoader```, set the ```batch_size=32```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Recreate model_2 used in this notebook capable of fitting on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_stack_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_shape, hidden_units, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_stack_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv_stack_3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_stack_4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_units*7*7, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack_1(x)\n",
    "        x = self.conv_stack_2(x)\n",
    "        x = self.conv_stack_3(x)\n",
    "        x = self.conv_stack_4(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Train the model you built in exercise 8 on CPU and GPU and see how long it takes on each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyvgg_cpu = TinyVGG(1, 10, len(train_data.classes)).to('cpu')\n",
    "tinyvgg_gpu = TinyVGG(1, 10, len(train_data.classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyvgg_cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiteacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "645d004dd1412f554783dad564b4ed89d297d59ce6765071e5ea4608253deff6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
