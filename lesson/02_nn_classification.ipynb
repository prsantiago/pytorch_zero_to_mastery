{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make classification data and get it ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Make 1000 samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03, # a little bit of noise to the dots\n",
    "                    random_state=42) # keep random state so we get the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'First 5 X features:\\n{X[:5]}')\n",
    "print(f'First 5 y labels:\\n{y[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame of circle data\n",
    "import pandas as pd\n",
    "\n",
    "circles = pd.DataFrame({\n",
    "    \"X1\": X[:, 0],\n",
    "    \"X2\": X[:, 1],\n",
    "    \"label\": y\n",
    "})\n",
    "circles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check different labels\n",
    "circles.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.PiYG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Input and output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of our features and labels\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first example of features and labels\n",
    "X_sample = X[0]\n",
    "y_sample = y[0]\n",
    "print(f'Values for one sample of X: {X_sample} and the same for y: {y_sample}')\n",
    "print(f'Shape for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Turn data into tensors and create train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into tensors\n",
    "# Otherwise this causes issues with computations later on\n",
    "import torch\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# View the first five samples\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=42) # make the random split reproducible\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Make device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Construct a model class that subclasses nn.Module\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n",
    "\n",
    "    # 3. Define a forward method containing the forward pass computation\n",
    "    def forward(self, x):        \n",
    "        # Return the output of layer_2, a single feature, the same shape as y\n",
    "        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n",
    "\n",
    "\n",
    "# 4. Create an instance of the model and send it to target device\n",
    "model_0 = CircleModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate CircleModelV0 with nn.Sequential\n",
    "model_0_seq = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1),\n",
    ").to(device)\n",
    "\n",
    "model_0_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the model\n",
    "untrained_preds = model_0(X_test.to(device))\n",
    "\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setup loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogits = sigmoid built-in\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Going from raw model outputs to predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 outputs of the forward pass on the test data\n",
    "y_logits = model_0(X_test.to(device))\n",
    "y_logits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sigmoid on model logits\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "y_pred_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the predicted labels (round the prediction probabilities)\n",
    "y_preds = torch.round(y_pred_probs)\n",
    "\n",
    "# In full\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))))\n",
    "\n",
    "# Check for equality\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze())[:5])\n",
    "\n",
    "# Get rid of extra dimension\n",
    "y_preds.squeeze()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Building a training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Building training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    train_logits = model_0(X_train).squeeze() # squeeze to remove extra '1' dimension, this won't work unless model anda data are on same device\n",
    "    train_preds = torch.round(torch.sigmoid(train_logits)) # turn logits -> pred probs -> pred labels\n",
    "\n",
    "    # 2. Calculate loss/accuracy\n",
    "    train_loss = loss_fn(train_logits, y_train) # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "    train_acc = accuracy_fn(y_train, train_preds)\n",
    "\n",
    "    # 3. Zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    train_loss.backward()\n",
    "\n",
    "    # 5. Step optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    if epoch % 10 == 0:\n",
    "        model_0.eval()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # 1. Forward pass\n",
    "            test_logits = model_0(X_test).squeeze()\n",
    "            test_preds = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "            # 2. Calculate loss/accuracy\n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_acc = accuracy_fn(y_test, test_preds)\n",
    "\n",
    "        # Print out what's happening every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss:.5f}, Accuracy: {train_acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Make predictions and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "\n",
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for traininig and test sets\n",
    "plt.figure(figsize =(8, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Train')\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Test')\n",
    "plot_decision_boundary(model_0, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Improving a model (from a model perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # three linear layers with input 2 output 1 and 10 hidden layers\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Creating a model like this is the same as belos, though below\n",
    "        # generally benefits from speedups where possible\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model_1 = CircleModelV1().to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss function with BCE with logits and SGD optimizer with 0.1 of lr\n",
    "# loss_fn = nn.BCELoss() # Requieres sigmoid on input\n",
    "loss_fn = nn.BCEWithLogitsLoss() # Does not requieres sigmoid on input\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 1000 # Train for longer\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    # 1. Forward\n",
    "    train_logits = model_1(X_train).squeeze()\n",
    "    train_pred = torch.round(torch.sigmoid(train_logits)) # logits -> prediction probabilities -> predictions label\n",
    "\n",
    "    # 2. Calculate loss/accuracy\n",
    "    train_loss = loss_fn(train_logits, y_train)\n",
    "    train_acc = accuracy_fn(y_train, train_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    train_loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        ### Testing\n",
    "        model_1.eval()\n",
    "        with torch.inference_mode():\n",
    "            # 1. Forward pass\n",
    "            test_logits = model_1(X_test).squeeze()\n",
    "            test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "            # 2. Calculate loss/accuracy\n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_acc = accuracy_fn(y_test, test_pred)\n",
    "\n",
    "        # Print out what's happening every 10 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f}, Train Accuracy: {train_acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for training and test\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_1, X_train, y_train)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_1, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Preparing data to see if our model can model a straight line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data (same as notebook 01)\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.01\n",
    "\n",
    "# Create data\n",
    "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y_regression = weight * X_regression + bias # linear regression formula\n",
    "\n",
    "# Check the data\n",
    "print(len(X_regression))\n",
    "X_regression[:5], y_regression[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test splits\n",
    "train_split = int(0.8 * len(X_regression)) # 80% of data used for training set\n",
    "X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n",
    "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
    "\n",
    "# Check the lengths of each split\n",
    "print(len(X_train_regression), \n",
    "    len(y_train_regression), \n",
    "    len(X_test_regression), \n",
    "    len(y_test_regression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_data=X_train_regression,\n",
    "    train_labels=y_train_regression,\n",
    "    test_data=X_test_regression,\n",
    "    test_labels=y_test_regression\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Adjusting ```model_1``` to fit a straight line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture as model_1 (but using sequential)\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(1,10),\n",
    "    nn.Linear(10,10),\n",
    "    nn.Linear(10, 1)\n",
    ").to(device)\n",
    "\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put data to target device\n",
    "X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\n",
    "X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    # 1. Forward pass\n",
    "    train_pred = model_2(X_train_regression)\n",
    "\n",
    "    # 2. Calculate loss/acc\n",
    "    train_loss = loss_fn(train_pred, y_train_regression)\n",
    "\n",
    "    # 3. Optim zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    train_loss.backward()\n",
    "\n",
    "    # 5. Optim step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        ### Testing\n",
    "        model_2.eval()\n",
    "        with torch.inference_mode():\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_2(X_test_regression)\n",
    "\n",
    "            # 2. Calculate loss/acc\n",
    "            test_loss = loss_fn(test_pred, y_test_regression)\n",
    "\n",
    "        # Print out what's happening every 10 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)\n",
    "# (try removing .cpu() from one of the below and see what happens)\n",
    "plot_predictions(train_data=X_train_regression.cpu(),\n",
    "                 train_labels=y_train_regression.cpu(),\n",
    "                 test_data=X_test_regression.cpu(),\n",
    "                 test_labels=y_test_regression.cpu(),\n",
    "                 predictions=test_pred.cpu());"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The missing piece: non-linearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Recreating non-linear data (red and blue circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and plot data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "X, y = make_circles(n_samples=n_samples, noise=0.03, random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors and split into train and test sets\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Turn data into tensors\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Building a model with non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with non-linear activation function\n",
    "from torch import nn\n",
    "\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(2, 10)\n",
    "        self.layer_2 = nn.Linear(10, 10)\n",
    "        self.layer_3 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU() # <- add in ReLu activation function\n",
    "\n",
    "        # Can also put sigmoid in the model\n",
    "        # This would mean you don't need to use it on the predictions\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Interspere the ReLu activation function between layers\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        return self.layer_3(x)\n",
    "\n",
    "model_3 = CircleModelV2().to(device)\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "epochs = 1000\n",
    "\n",
    "# Send data to save device as model\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Testing\n",
    "\n",
    "    # 1. Forward pass\n",
    "    train_logits = model_3(X_train).squeeze()\n",
    "    train_preds = torch.round(torch.sigmoid(train_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "    # 2. Calculate loss and acc \n",
    "    train_loss = loss_fn(train_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n",
    "    train_acc = accuracy_fn(y_train, train_preds)\n",
    "\n",
    "    # 3. Optim zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    train_loss.backward()\n",
    "\n",
    "    # 5. Optim step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_3.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_3(X_test).squeeze()\n",
    "        test_preds = torch.round(torch.sigmoid(test_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "        # 2 Calculate loss and acc\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_test, test_preds)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f}, Train Accuracy: {train_acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for training and test sets\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_1, X_test, y_test) # model_1 = no non-linearity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Replicating non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy tensor (similar to the data going into our model(s))\n",
    "A = torch.arange(-10, 10, 1, dtype=torch.float32)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the toy tensor\n",
    "plt.plot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ReLu function by hand\n",
    "def relu(x):\n",
    "    return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n",
    "\n",
    "# Pass toy tensor through ReLu function\n",
    "relu(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ReLu activated toy tensor\n",
    "plt.plot(relu(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# Test custom sigmoid on toy tensor\n",
    "sigmoid(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sigmoid activated toy tensor\n",
    "plt.plot(sigmoid(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Putting things together by building a multi-class PyTorch model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Creating multi-class classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters for data creation\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# 1. Create multi-class data\n",
    "X_blob, y_blob = make_blobs(n_samples=1000, \n",
    "                            n_features=NUM_FEATURES, # X features \n",
    "                            centers=NUM_CLASSES, # y labels\n",
    "                            cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n",
    "                            random_state=RANDOM_SEED)\n",
    "\n",
    "# 2. Turn data into tensors\n",
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "print(X_blob[:5], y_blob[:5])\n",
    "\n",
    "# 3. Split into train and test sets\n",
    "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# 4. Plot data\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Building a multi-class classification model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Build model\n",
    "class BlobModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_features=8) -> None:\n",
    "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
    "\n",
    "        Args:\n",
    "            input_features (int): Number of input features to the model.\n",
    "            ouput_features (int): Number of output features of the model (how many classes there are).\n",
    "            hidden_units (int): Number of hidden units between layers, default: 8\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(input_features, hidden_features)\n",
    "        self.linear_layer_2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.linear_layer_3 = nn.Linear(hidden_features, output_features)\n",
    "        self.relu = nn.ReLU() # Does out dataset require non-linear layers?\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_layer_3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "model_4 = BlobModel(NUM_FEATURES, NUM_CLASSES, 8).to(device)\n",
    "\n",
    "model_4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Creating a loss function and optimizer for a multi-class PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_4.parameters(), lr=0.1) # experiment with different values for lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Getting prediction probabilities for a multi-class PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\n",
    "model_4(X_blob_train.to(device))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many elements in a single prediction sample?\n",
    "model_4(X_blob_train.to(device))[0].shape, NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction logits with model\n",
    "y_logits = model_4(X_blob_test.to(device))\n",
    "print(y_logits[:5])\n",
    "\n",
    "# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "print(y_pred_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the first sample output of the softmax activation function\n",
    "torch.sum(y_pred_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which class does the model think is *most* likely at the index 0 sample?\n",
    "print(y_pred_probs[0])\n",
    "print(torch.argmax(y_pred_probs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Creating a training and testing loop for a multi-class PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put the data to target device\n",
    "X_blob_train, X_blob_test = X_blob_train.to(device), X_blob_test.to(device)\n",
    "y_blob_train, y_blob_test = y_blob_train.to(device), y_blob_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    # model_4.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    train_logits = model_4(X_blob_train)\n",
    "    train_preds = torch.softmax(train_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "    # 2. Calculate loss\n",
    "    train_loss = loss_fn(train_logits, y_blob_train)\n",
    "    train_acc = accuracy_fn(y_blob_train, train_preds)\n",
    "\n",
    "    # 3. Optim zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    train_loss.backward()\n",
    "\n",
    "    # 5. Optim step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Test\n",
    "    if epoch % 10 == 0: # eval cada 10 epochs o cada epoch solo que imprimir cada diez epochs\n",
    "        model_4.eval() # cambiar a antes o despues\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            test_logits = model_4(X_blob_test)\n",
    "            test_preds = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "            test_loss = loss_fn(test_logits, y_blob_test)\n",
    "            test_acc = accuracy_fn(y_blob_test, test_preds)\n",
    "\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Making and evaluating predictions with a PyTorch multi-class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_4.eval()\n",
    "with torch.inference_mode():\n",
    "    pred_logits = model_4(X_blob_test)\n",
    "\n",
    "# View the first 10 predictions\n",
    "pred_logits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn predicted logits in prediction probabilities\n",
    "pred_probs = torch.softmax(pred_logits, dim=1)\n",
    "\n",
    "# Turn predicitons probabilities into prediction labels\n",
    "pred_labels = pred_probs.argmax(dim=1)\n",
    "\n",
    "# The above operations can be simplified to simply pass to pred_labels, without having the probabilities\n",
    "# pred_labels = torch.argmax(pred_logits, dim=1)\n",
    "\n",
    "# Compare first 10 model preds and test labels\n",
    "print(f\"Predictions: {pred_labels[:10]}\\nLabels: {y_blob_test[:10]}\")\n",
    "print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=pred_labels)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_4, X_blob_train, y_blob_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_4, X_blob_test, y_blob_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. More classification evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Setup metric and make sure it's on the target device\n",
    "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)\n",
    "\n",
    "# Calculate accuracy\n",
    "torchmetrics_accuracy(pred_labels, y_blob_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('aiteacher')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "645d004dd1412f554783dad564b4ed89d297d59ce6765071e5ea4608253deff6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
